# **Papers on NN Compression**
## **Paper list**
### **CVPR 2018**
* [**CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.pdf)</br>
* [**"Learning-Compression" Algorithms for Neural Net Pruning**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.pdf)
* [**NISP: Pruning Networks Using Neuron Importance Score Propagation**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_NISP_Pruning_Networks_CVPR_2018_paper.pdf)
* [**Wide Compression: Tensor Ring Nets**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Wide_Compression_Tensor_CVPR_2018_paper.pdf)
* [**NestedNet: Learning Nested Sparse Structures in Deep Neural Networks**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_NestedNet_Learning_Nested_CVPR_2018_paper.pdf)
* [**SBNet: Sparse Blocks Network for Fast Inference**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.pdf)
* [**Interleaved Structured Sparse Convolutional Neural Networks**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.pdf)

### **NIPS 2018**
* [**Discrimination-aware Channel Pruning for Deep Neural Networks**](http://papers.nips.cc/paper/7367-discrimination-aware-channel-pruning-for-deep-neural-networks.pdf)
* [**Frequency-Domain Dynamic Pruning for Convolutional Neural Networks**](http://papers.nips.cc/paper/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks.pdf)
* [**Paraphrasing Complex Network: Network Compression via Factor Transfer**](http://papers.nips.cc/paper/7541-paraphrasing-complex-network-network-compression-via-factor-transfer.pdf)
* [**Learning Compressed Transforms with Low Displacement Rank**](http://papers.nips.cc/paper/8119-learning-compressed-transforms-with-low-displacement-rank.pdf)
* [**Sparse DNNs with Improved Adversarial Robustness**](http://papers.nips.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness.pdf)
* [**Posterior Concentration for Sparse Deep Learning**](http://papers.nips.cc/paper/7372-posterior-concentration-for-sparse-deep-learning.pdf)
* [**Boosted Sparse and Low-Rank Tensor Regression**](http://papers.nips.cc/paper/7379-boosted-sparse-and-low-rank-tensor-regression.pdf)
* [**Gradient Sparsification for Communication-Efficient Distributed Optimization**](http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf)
* [**Learning sparse neural networks via sensitivity-driven regularization**](http://papers.nips.cc/paper/7644-learning-sparse-neural-networks-via-sensitivity-driven-regularization.pdf)
* [**TETRIS: TilE-matching the TRemendous Irregular Sparsity**](http://papers.nips.cc/paper/7666-tetris-tile-matching-the-tremendous-irregular-sparsity.pdf)

### **ICLR 2018**
* [**Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers**](https://openreview.net/pdf?id=HJ94fqApW)
* [**Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio**](https://openreview.net/forum?id=S1D8MPxA-)
* [**Model compression via distillation and quantization**](https://openreview.net/pdf?id=S1XolQbRW)
* [**N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning**](https://openreview.net/pdf?id=B1hcZZ-AW)
* [**LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING**](https://openreview.net/pdf?id=rypT3fb0b)
* [**Learning Intrinsic Sparse Structures within Long Short-Term Memory**](https://openreview.net/pdf?id=rk6cfpRjZ)
* [**Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip**](https://openreview.net/pdf?id=HkxF5RgC-)
* [**Learning Sparse Neural Networks through L_0 Regularization**](https://openreview.net/forum?id=H1Y8hhg0b)
* [**Deep Rewiring: Training very sparse deep networks**](https://openreview.net/pdf?id=BJ_wN01C-)
* [**Efficient Sparse-Winograd Convolutional Neural Networks**](https://openreview.net/pdf?id=HJzgZ3JCW)

### **ECCV 2018**
* [**A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Tianyun_Zhang_A_Systematic_DNN_ECCV_2018_paper.pdf)
* [**Coreset-Based Neural Network Compression**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Abhimanyu_Dubey_Coreset-Based_Convolutional_Neural_ECCV_2018_paper.pdf)
* [**AMC: AutoML for Model Compression and Acceleration on Mobile Devices**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.pdf)
* [**Clustering Convolutional Kernels to Compress Deep Neural Networks**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Son_Clustering_Kernels_for_ECCV_2018_paper.pdf)
* [**Extreme Network Compression via Filter Group Approximation**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Bo_Peng_Extreme_Network_Compression_ECCV_2018_paper.pdf)
* [**Constraint-Aware Deep Neural Network Compression**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Changan_Chen_Constraints_Matter_in_ECCV_2018_paper.pdf)
* [**Data-Driven Sparse Structure Selection for Deep Neural Networks**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Zehao_Huang_Data-Driven_Sparse_Structure_ECCV_2018_paper.pdf)

### **ICML 2018**
* [**Compressing Neural Networks using the Variational Information Bottleneck**](http://proceedings.mlr.press/v80/dai18d/dai18d.pdf)
* [**Weightless: Lossy weight encoding for deep neural network compression**](http://proceedings.mlr.press/v80/reagan18a/reagan18a.pdf)
* [**Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions**](http://proceedings.mlr.press/v80/wu18h/wu18h.pdf)

### **AAAI 2018**
* [**Auto-Balanced Filter Pruning for Efficient Convolutional Neural Networks**](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16450/16263)
* [**Deep Neural Network Compression With Single and Multiple Level Quantization**](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16479/16742)
* [**Beyond Sparsity: Tree Regularization of Deep Models for Interpretability**](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16285/15867)

### **IJCAI 2018**
* [**Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks**](https://www.ijcai.org/proceedings/2018/309)
* [**Efficient DNN Neuron Pruning by Minimizing Layer-wise Nonlinear Reconstruction Error**](https://www.ijcai.org/proceedings/2018/318)
* [**Optimization based Layer-wise Magnitude-based Pruning for DNN Compression**](https://www.ijcai.org/proceedings/2018/330)
* [**Accelerating Convolutional Networks via Global & Dynamic Filter Pruning**](https://www.ijcai.org/proceedings/2018/336)
* [**Where to Prune: Using LSTM to Guide End-to-end Pruning**](https://www.ijcai.org/proceedings/2018/445)
* [**Dynamically Hierarchy Revolution: DirNet for Compressing Recurrent Neural Network on Mobile Devices**](https://www.ijcai.org/proceedings/2018/429)
* [**Improving Deep Neural Network Sparsity through Decorrelation Regularization**](https://www.ijcai.org/proceedings/2018/453)
* [**Building Sparse Deep Feedforward Networks using Tree Receptive Fields**](https://www.ijcai.org/proceedings/2018/700)
</br>

**To be updated**